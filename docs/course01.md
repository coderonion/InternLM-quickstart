### 第一课 书生·浦语大模型全链路开源体系

#### 一、大模型成为发展通用人工智能的重要途径

 - 专用模型：针对特定任务，一个模型解决一个问题。
   - 2006年，深度学习理论突破（深度置信网络）。
   - 2011年，大规模语音识别。
   - 2012年，图像识别领域的ImageNet竞赛。
   - 2014年，人脸识别（LFW数据集上人脸识别率超过人类）。
   - 2016年，围棋比赛，AlphaGo战胜李世石。
   - 2019年，德州扑克首次在多人复杂对局转给你超越人类。
   - 2021年，AlphaFold蛋白质结构预测准确率新高。
 - 通用大模型：一个模型应对多种任务、多种模态。
   	1. 2023年，OpenAI的ChatGPT发布。

![](./imgs/001.png)

#### 二、书生·浦语大模型开源历程

 - 2023年6月7日，[InternLM](https://github.com/InternLM/InternLM)千亿参数语言大模型发布。
 - 2023年7月6日，InternLM千亿参数语言大模型全面升级，支持8K语境、26种语言。全面开源、免费商用：InternLM-7B、全链条开源工具体系。
 - 2023年8月14日，[书生·万卷1.0](https://github.com/opendatalab/WanJuan1.0)多模态预训练语料库开源发布。
 - 2023年8月21日，升级版对话模型InternLM-Chat-7B v1.1发布，开源智能体框架Lagent，支持从语言模型到智能体升级转换。
 - 2023年8月28日，InternLM 千亿参数模型的参数量升级到123B。
 - 2023年9月20日，增强版InternLM-20B开源，开源工具链全线升级。
 - 2024年1月17日，书生·浦语 2.0 (InterLM2) 开源。

![](./imgs/002.png)

#### 二、书生·浦语2.0（InternLM2)

##### （一）模型规格

- 7B模型：为轻量级的研究和应用提供了一个轻便但性能不俗的模型。
- 20B模型：模型的综合性能更为强劲，可有效支持更加复杂的实用场景。

##### （二）模型版本：面向不同的实用需求，每个规格包含三个模型版本。

- InternLM2-Base：高质量和具有很强可塑性的模型基座，是模型进行深度领域适配的高质量起点。
- InternLm2：在Base基础上，在多个能力方向进行了强化，在评测中成绩优异，同时保持了很好的通用语言能力，是我们推荐的在大部分应用中考虑选用的优秀基座。
- InternLM2-Chat：在Base基础上，经过SFT和RLHF，面相对话交互进行了优化，具有很好的指令遵循、共情聊天和调用工具等的能力。

##### （三）技术特色及亮点

- 新一代数据清洗过滤技术
  - 多维度数据价值评估：基于文本质量、信息质量、信息密度等维度对数据价值进行综合评估与提升。
  - 高质量语料驱动的数据富集：利用高质量语料的特征从物理世界、互联网以及语料库中进一步富集更多类似语料。
  - 有针对性的数据补齐：针对性补充语料，重点加强世界知识、数理、代码等核心能力。


- 超长上下文：模型在20万token上下文中，几乎完美实现“大海捞针”。
- 综合性能全面提升：推理、数学、代码提升显著，InternLM2-Chat-20B在重点评测上比肩ChatGPT（GPT-3.5）。
- 优秀的对话和创作体验：精准指令跟随，丰富的结构化创作，在AlpacaEval2超越GPT-3.5和Gemini Pro。
- 工具调用能力整体升级：可靠支持工具多轮调用，复杂智能体搭建。
- 突出的数理能力和实用的数据分析功能：强大的内生计算能力，加入代码解释后，在GSM8K和MATH达到和GPT-4相仿水平。

##### （四）从模型到应用典型流程

![](./imgs/003.png)

